# Definitions used in testing and quality assurance

## A
- **Accessibility Testing**: Ensures the software can be used by people with disabilities. This is dynamic validation testing for accessibility standards.
- **Acceptance Testing**: Determines if the software meets the acceptance criteria and is ready for release. This is dynamic validation testing, often performed by the end-users.
- **Ad-hoc Testing**: Similar to exploratory testing but without any specific purpose. It's a form of dynamic validation testing.
- **Alpha Testing**: Conducted by the internal teams after the development phase. It's dynamic validation testing in a controlled environment.
- **API Testing**: Validates the application programming interfaces. It's dynamic validation testing for backend services.
- **A/B Testing**: Compares two versions of the software to determine which one performs better. This is dynamic validation testing for optimization.

## B
- **Backward Compatibility Testing**: Ensures new versions of the software remain compatible with older versions. This is dynamic validation testing for version compatibility.
- **Beta Testing**: Performed by real users in a real environment before the software release. This is dynamic validation testing.
- **Black Box Testing**: A software testing method in which the tester does not have knowledge of the internal structure, design, or implementation of the item being tested. The focus is solely on testing the software's functionality based on its requirements.
- **Boundary Testing**: Tests the behavior of the software at boundary limits. This is dynamic validation testing for input boundaries.
- **Boundary Value Analysis**: A method of testing the software using input values at the boundary limits.
- **Branch Coverage**: A measure that ensures each branch (decision) in the software is executed at least once.
- **Bug**: A flaw in the software code causing it to produce unexpected results or behave in unintended ways.

## C
- **Chaos Engineering**: A discipline in software engineering that focuses on improving the resilience and reliability of systems by intentionally introducing failures to uncover weaknesses.
- **CI (Continuous Integration)**: The practice of merging all developers' working copies to a shared mainline several times a day.
- **CD (Continuous Deployment)**: The practice of automatically deploying new changes to production, ensuring that software can be reliably released at any time.
- **Compatibility Testing**: Checks the software's compatibility with different environments (OS, browsers, devices). This is dynamic validation testing.
- **Compliance Testing**: Ensures the software adheres to specific standards or regulations. This is dynamic validation testing for regulatory compliance.
- **Configuration Testing**: Validates the software's behavior under different configurations. This is dynamic validation testing for configuration scenarios.
- **Content Testing**: Validates the content for its correctness, readability, and usability. This is dynamic validation testing for content accuracy.
- **Conformance Testing**: Validates the software against defined standards or specifications. This is dynamic validation testing against standards.

## D
- **Decision Table Testing**: A method of testing the software's behavior for different input combinations using a table format.
- **Dependability**: The quality of a delivered service on which reliance can justifiably be placed.
- **Dummy**: Objects passed around but never used.
- **Dynamic Testing**: Tests the software by executing it. This involves running the software and validating its behavior.

## E
- **Effective Error**: An error that results in a noticeable system failure or deviation from expected behavior. Unlike latent errors, effective errors manifest as active failures.
- **Equivalence Partitioning**: A method of dividing the input data of a software unit into partitions of equivalent data from which test cases can be derived.
- **Error**: A part of the system or service that can lead to failure, often related to a segment of the software product's code.

## F
- **Fake**: Working implementations with shortcuts unsuitable for production.
- **Failure**: The unwanted effects that occur when the delivered service deviates from the specification of the expected service. It is the inability of the software to perform as intended.
- **Fault/Defect/Bug**: The root cause of an error. Faults can arise from poor requirements specification, design flaws, manufacturing defects, hardware wear and tear, and coding mistakes in software, and usually lead to unexpected behavior.

## G
- **Grey Box Testing**: A software testing method that combines both black box and white box testing techniques. In this method, the tester has partial knowledge of the software's internal workings.

## L
- **Latent Error**: An error that exists but does not necessarily cause a failure. It remains dormant until triggered by executing the portion of the code where it resides, resulting in failure.
- **Loop Testing**: A method of testing the loops in the software to ensure they function as expected.

## M
- **Mock**: Objects pre-programmed with expectations regarding their method calls.
- **Matrix Testing**: A method of identifying all the variables of an application and then testing them in pairs.

## O
- **Oracle**: In software testing, an oracle refers to a mechanism or method used to determine whether a test has passed or failed. It provides the expected outcome for a given test case.
- **Orthogonal Array Testing**: A systematic, statistical method of testing that uses orthogonal arrays to ensure all combinations of variables are tested with a minimal set of test cases.

## P
- **Path Coverage**: A measure that ensures all paths in the software are executed at least once.
- **Pattern Testing**: A method of testing the software based on predefined patterns.

## R
- **Regression Testing**: A method of ensuring that new changes in the software have not adversely affected the existing functionalities.
- **Reliability**: The probability that an end user will not encounter a fault leading to an error and subsequent failure.

## S
- **SDLC (Software Development Life Cycle)**: A systematic process for planning, creating, testing, deploying, and maintaining software applications or systems. It encompasses various phases, including requirements gathering, system design, coding, testing, deployment, and maintenance.
- **Service**: The system behavior as perceived by the user.
- **Spy**: Stubs that record information based on how they were called.
- **State Transition Testing**: A method of testing the software's behavior for different states and the transitions between those states.
- **Statement Coverage**: A measure that ensures each statement in the software is executed at least once.
- **Stub**: A minimal implementation of an interface that can be used to satisfy a system's dependency.
- **System**: A combination of interacting elements organized to achieve one or more stated purposes.

## T
- **Test Case**: A set of conditions or variables under which a tester will determine whether a system under test satisfies requirements or works correctly.
- **Test Scenario**: A high-level description of what needs to be tested in terms of functionality.
- **Test Suite**: A collection of test cases that are intended to be used to test a software program to show that it has some specified set of behaviors.

## U
- **Unit Testing**: A method of testing individual units or components of a software. It is often automated but can also be done manually.

## V
- **Validation**: The process of evaluating software during or at the end of the development process to determine whether it satisfies specified requirements.
- **Verification**: The process of evaluating a system or component to determine whether the products of a given development phase satisfy the conditions imposed at the start of that phase.

## W
- **White Box Testing**: A software testing method in which the tester has complete knowledge of the internal workings of the item being tested. It focuses on checking the flow of inputs and outputs through the application, improving design and usability, strengthening security, and ensuring all functionalities work as expected.
