# Definitions used in testing and quality assurance

## B
**Black Box Testing**: A software testing method in which the tester does not have knowledge of the internal structure, design, or implementation of the item being tested. The focus is solely on testing the software's functionality based on its requirements.

**Boundary Value Analysis**: A method of testing the software using input values at the boundary limits.

**Branch Coverage**: A measure that ensures each branch (decision) in the software is executed at least once.

## C
**Chaos Engineering**: A discipline in software engineering that focuses on improving the resilience and reliability of systems by intentionally introducing failures to uncover weaknesses.

**CI (Continuous Integration)**: The practice of merging all developers' working copies to a shared mainline several times a day.

**CD (Continuous Deployment)**: The practice of automatically deploying new changes to production, ensuring that software can be reliably released at any time.

## D
**Decision Table Testing**: A method of testing the software's behavior for different input combinations using a table format.

**Dependability**: The quality of a delivered service on which reliance can justifiably be placed.

**Dummy**: Objects passed around but never used.

## E
**Effective Error**: An error that results in a noticeable system failure or deviation from expected behavior. Unlike latent errors, effective errors manifest as active failures.

**Equivalence Partitioning**: A method of dividing the input data of a software unit into partitions of equivalent data from which test cases can be derived.

**Error**: A part of the system or service that can lead to failure, often related to a segment of the software product's code.

## F
**Fake**: Working implementations with shortcuts unsuitable for production.

**Failure**: The unwanted effects that occur when the delivered service deviates from the specification of the expected service. It is the inability of the software to perform as intended.

**Fault/Defect/Bug**: The root cause of an error. Faults can arise from poor requirements specification, design flaws, manufacturing defects, hardware wear and tear, and coding mistakes in software, and usually lead to unexpected behavior.

## G
**Grey Box Testing**: A software testing method that combines both black box and white box testing techniques. In this method, the tester has partial knowledge of the software's internal workings.

## L
**Latent Error**: An error that exists but does not necessarily cause a failure. It remains dormant until triggered by executing the portion of the code where it resides, resulting in failure.

**Loop Testing**: A method of testing the loops in the software to ensure they function as expected.

## M
**Mock**: Objects pre-programmed with expectations regarding their method calls.

**Matrix Testing**: A method of identifying all the variables of an application and then testing them in pairs.

## O
**Oracle**: In software testing, an oracle refers to a mechanism or method used to determine whether a test has passed or failed. It provides the expected outcome for a given test case.

**Orthogonal Array Testing**: A systematic, statistical method of testing that uses orthogonal arrays to ensure all combinations of variables are tested with a minimal set of test cases.

## P
**Path Coverage**: A measure that ensures all paths in the software are executed at least once.

**Pattern Testing**: A method of testing the software based on predefined patterns.

## R
**Regression Testing**: A method of ensuring that new changes in the software have not adversely affected the existing functionalities.

**Reliability**: The probability that an end user will not encounter a fault leading to an error and subsequent failure.

## S
**SDLC (Software Development Life Cycle)**: A systematic process for planning, creating, testing, deploying, and maintaining software applications or systems. It encompasses various phases, including requirements gathering, system design, coding, testing, deployment, and maintenance.

**Service**: The system behavior as perceived by the user.

**Spy**: Stubs that record information based on how they were called.

**State Transition Testing**: A method of testing the software's behavior for different states and the transitions between those states.

**Statement Coverage**: A measure that ensures each statement in the software is executed at least once.

**Stub**: Provides canned answers to calls made during the test.

## T
**Test**: A procedure designed to determine whether a specific part of a software application functions as intended. It evaluates the software's actual behavior against its expected behavior.

**Test Double**: A placeholder or substitute used in unit testing for the actual components of a system.

## W
**White Box (or Glassbox) Testing**: A software testing method in which the tester is aware of the internal structure, design, and implementation of the item being tested. The focus is on checking the internal logic, pathways, and code structure of the software.
