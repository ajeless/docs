# Definitions used in testing and quality assurance

## A
- **Accessibility Testing:** Ensures the software can be used by people with disabilities. This is dynamic validation testing for accessibility standards.
- **Acceptance Testing:** Determines if the software meets the acceptance criteria and is ready for release. This is dynamic validation testing, often performed by the end-users.
- **Ad-hoc Testing:** Similar to exploratory testing but without any specific purpose. It's a form of dynamic validation testing.
- **Alpha Testing:** Conducted by the internal teams after the development phase. It's dynamic validation testing in a controlled environment.
- **API Testing:** Validates the application programming interfaces. It's dynamic validation testing for backend services.
- **A/B Testing:** Compares two versions of the software to determine which one performs better. This is dynamic validation testing for optimization.
- **Availability:** The readiness of the software to respond to user requests.  $$Availability = MTBF/(MTBF + MTTR)$$  


## B
- **Backward Compatibility Testing:** Ensures new versions of the software remain compatible with older versions. This is dynamic validation testing for version compatibility.
- **Beta Testing:** Performed by real users in a real environment before the software release. This is dynamic validation testing.
- **Black Box Testing:** A software testing method in which the tester does not have knowledge of the internal structure, design, or implementation of the item being tested. The focus is solely on testing the software's functionality based on its requirements.
- **Boundary Value Analysis:** Tests the behavior of the software at boundary limits. This is dynamic validation testing for input boundaries.
- **Branch Coverage:** A measure that ensures each branch (decision) in the software is executed at least once.
- **Bug:** A flaw in the software code causing it to produce unexpected results or behave in unintended ways.

## C
- **Chaos Engineering:** A discipline in software engineering that focuses on improving the resilience and reliability of systems by intentionally introducing failures to uncover weaknesses.
- **CI (Continuous Integration):** The practice of merging all developers' working copies to a shared mainline several times a day.
- **CD (Continuous Deployment):** The practice of automatically deploying new changes to production, ensuring that software can be reliably released at any time.
- **Compatibility Testing:** Checks the software's compatibility with different environments (OS, browsers, devices). This is dynamic validation testing.
- **Compliance Testing:** Ensures the software adheres to specific standards or regulations. This is dynamic validation testing for regulatory compliance.
- **Configuration Testing:** Validates the software's behavior under different configurations. This is dynamic validation testing for configuration scenarios.
- **Content Testing:** Validates the content for its correctness, readability, and usability. This is dynamic validation testing for content accuracy.
- **Conformance Testing:** Validates the software against defined standards or specifications. This is dynamic validation testing against standards.

## D
- **Decision Table Testing:** A method of testing the software's behavior for different input combinations using a table format.
- **Dependability:** The quality of a delivered service on which reliance can justifiably be placed.
- **Dummy:** Objects passed around but never used.
- **Dynamic Testing:** Tests the software by executing it. This involves running the software and validating its behavior.

## E
- **Effective Error:** An error that results in a noticeable system failure or deviation from expected behavior. Unlike latent errors, effective errors manifest as active failures.
- **Equivalence Partitioning:** A method of dividing the input data of a software unit into partitions of equivalent data from which test cases can be derived.
- **Error:** A part of the system or service that can lead to failure, often related to a segment of the software product's code.
- **Error Forecasting:** A way of measuring how likely we are to have failures based on the behavior of the program.
- **Error Removal:** The process of getting rid of the errors themselves, often through verification.

## F
- **Fake:** Working implementations with shortcuts unsuitable for production.
- **Failure:** The unwanted effects that occur when the delivered service deviates from the specification of the expected service. It is the inability of the software to perform as intended.
- **Fault Avoidance:** Preventing, by construction, certain kinds of faults. Some programming languages have built-in fault avoidance mechanisms.
- **Fault Tolerance:** Having redundant subsystems such that one of them can fail and the rest will continue to operate.
- **Fault/Defect/Bug:** The root cause of an error. Faults can arise from poor requirements specification, design flaws, manufacturing defects, hardware wear and tear, and coding mistakes in software, and usually lead to unexpected behavior.

## G
- **Grey Box Testing:** A software testing method that combines both black box and white box testing techniques. In this method, the tester has partial knowledge of the software's internal workings.

## I
- **Integrity:** The absence of improper system alteration.
- **Integration Testing:** Validates the interactions between different software modules. This is dynamic validation testing for module interactions.
- **Interface Testing:** Validates the software's interfaces, including UI and API. This is dynamic validation testing for interfaces.

## L
- **Latent Error:** An error that exists in the system but does not result in a failure until certain conditions are met.
- **Load Testing:** Validates the software's behavior under expected loads. This is dynamic validation testing for system performance under load.
- **Localization Testing:** Validates the software's behavior for a specific locale or culture. This is dynamic validation testing for localization.

## M
- **Maintainability:** The ability for a process to undergo modifications and repairs.
- **Manual Testing:** Testing the software manually without the use of automated tools. This is dynamic validation testing performed by testers.
- **Mock:** Objects with pre-defined answers to method calls.
- **Monkey Testing:** A random testing process where the system is tested by providing random inputs and observing the behavior.
- **MTBF (Mean Time Between Failures):** The average time interval between two successive failures of a system or component during its operation. It is a measure of the reliability of a system and is calculated as MTBF = Total operating time / Number of failures.
- **MTTR (Mean Time To Recover or Mean Time To Repair):** The average time taken to restore a system or component to its operational state after a failure. It measures the efficiency of the repair process and is calculated as MTTR = Total downtime / Number of failures.
- **Mutation Testing:** A method of software testing where certain statements of the source code are changed/mutated to check if the test cases are able to find the errors.

## P
- **Performance Testing:** Validates the software's performance characteristics. This is dynamic validation testing for performance metrics.
- **Positive Testing:** Tests the software's behavior with valid input data. This is dynamic validation testing for positive scenarios.
- **Post-mortem Testing:** A process of analyzing the software after it has been executed to determine the causes of any failures.

## R
- **Recoverability:** How quickly, if the system fails, it can be restored to correct operation.
- **Regression Testing:** Ensures that new changes in the software do not affect the existing functionality. This is dynamic validation testing for regression.
- **Reliability:** Continuity of correct service.
- **Robustness:** The ability of a system to handle and recover from unexpected inputs or conditions.

## S
- **Safety:** The absence of catastrophic consequences based on failures of the software.
- **Sanity Testing:** A subset of regression testing to determine if a section of the application is still working after a minor change.
- **Security Testing:** Validates the software's security features. This is dynamic validation testing for security vulnerabilities.
- **Smoke Testing:** Preliminary testing to check the software's basic functionality. This is dynamic validation testing for basic functionality.
- **Spy:** Objects that store arguments for method calls.
- **Static Testing:** Validates the software without executing it. This involves reviewing the software's code, requirements, and design.
- **Stress Testing:** Validates the software's behavior under extreme conditions. This is dynamic validation testing for system performance under stress.
- **Stub:** Used to simulate the behavior of modules/ components.

## U
- **Unit Testing:** Validates individual units or components of the software. This is dynamic validation testing for individual units.
- **Usability Testing:** Validates the software's usability aspects. This is dynamic validation testing for user experience.

## V
- **Validation:** The process of evaluating software at the end of the development process to ensure compliance with software requirements.
- **Verification:** The process of evaluating software during or at the end of the development process to ensure it satisfies specified requirements in a specific context.
- **Volume Testing:** Validates the software's behavior with a large amount of data. This is dynamic validation testing for data volume handling.

## W
- **White Box Testing:** A software testing method in which the tester has knowledge of the internal workings, structure, and design of the item being tested. The focus is on testing the software's logic paths, code statements, branches, and internal logic.
